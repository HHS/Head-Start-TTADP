version: 2.1
orbs:
  node: circleci/node@5.0.2
executors:
  docker-executor:
    # for docker you must specify an image to use for the primary container
    docker:
      - image: cimg/node:18.19.1-browsers
  docker-postgres-executor:
    docker:
      - image: cimg/node:18.19.1-browsers
        environment:
          DATABASE_URL: postgresql://postgres@localhost/ttasmarthub
      - image: circleci/postgres:12.4-ram
        environment:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: secretpass
          POSTGRES_DB: ttasmarthub
  docker-python-executor:
    docker:
      - image: cimg/python:3.9.18
  docker-postgres-elasticsearch-executor:
    docker:
      - image: cimg/node:18.19.1-browsers
        environment:
          DATABASE_URL: postgresql://postgres@localhost/ttasmarthub
      - image: circleci/postgres:12.4-ram
        environment:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: secretpass
          POSTGRES_DB: ttasmarthub
      - image: opensearchproject/opensearch:1.3.2
        name: opensearch
        environment:
          transport.host: 0.0.0.0
          network.host: 0.0.0.0
          http.port: 9200
          cluster.name: opensearch-cluster
          node.name: opensearch-node1
          discovery.type: single-node
          DISABLE_SECURITY_PLUGIN: true
          ES_JAVA_OPTS: "-Xms256m -Xmx256m"
  machine-executor:
    machine:
      image: ubuntu-2204:current
  aws-executor:
    docker:
      - image: cimg/aws:2024.03
commands:
  create_combined_yarnlock:
    description: "Concatenate all yarn.json files into single file.
      File is used as checksum source for part of caching key."
    parameters:
      filename:
        type: string
        default: "combined-yarnlock.txt"
    steps:
      - run:
          name: Combine package-lock.json files to single file
          command: cat yarn.lock frontend/yarn.lock packages/common/yarn.lock > << parameters.filename >>
  create_combined_pipfreeze:
    description: "Concatenate all requirements.txt files into a single file. File is used as checksum source for part of caching key."
    parameters:
      filename:
        type: string
        default: "combined-requirements.txt"
    steps:
      - run:
          name: Combine requirements.txt files to single file
          command: cat similarity_api/src/requirements.txt > << parameters.filename >>
  notify_new_relic:
    description: "Notify new relic of a deploy"
    parameters:
      env_name:
        description: "Name of the environment. Ex. sandbox, dev, staging, prod"
        type: string
      new_relic_app_id:
        description: "App ID used in New Relic"
        type: string
      new_relic_api_key:
        description: "API key from New Relic"
        type: string
    steps:
      - run:
          name: Notify New Relic
          command: |
            curl -X POST "https://api.newrelic.com/v2/applications/<< parameters.new_relic_app_id >>/deployments.json" \
            -H "X-Api-Key: << parameters.new_relic_api_key >>" -i \
            -H "Content-Type: application/json" \
            -d \
            "{
              \"deployment\": {
                \"revision\": \"<< pipeline.git.revision >>\",
                \"description\": \"<< parameters.env_name >> Successfully Deployed\"
              }
            }"
  notify_slack:
    description: "Notify slack of a deploy to production"
    parameters:
      slack_bot_token:
        description: "Slack bot token"
        type: string
      slack_channel:
        description: "Slack channel name to post the message to"
        type: string
    steps:
    - run:
        name: Notify Slack of Deployment
        command: |
          # Check if the CIRCLE_PULL_REQUEST variable is set and extract the PR number from it
          if [ ! -z "$CIRCLE_PULL_REQUEST" ]; then
            PR_NUMBER=${CIRCLE_PULL_REQUEST##*/}
            MESSAGE_TEXT=":rocket: Deployment of PR <$CIRCLE_PULL_REQUEST|$PR_NUMBER> to production was successful!"
          else
            MESSAGE_TEXT=":rocket: Deployment to production was successful!"
          fi

          curl -X POST -H "Authorization: Bearer << parameters.slack_bot_token >>" \
          -H 'Content-type: application/json;charset=utf-8' \
          --data "{
            \"channel\": \"<< parameters.slack_channel >>\",
            \"text\": \"$MESSAGE_TEXT\"
          }" https://slack.com/api/chat.postMessage

  cf_deploy:
    description: "Login to cloud foundry space with service account credentials
      and push application using deployment configuration file."
    parameters:
      app_name:
        description: "Name of Cloud Foundry cloud.gov application; must match
          application name specified in manifest"
        type: string
      auth_client_id:
        description: "Name of CircleCi project environment variable that
          holds authentication client id, a required application variable"
        type: env_var_name
      auth_client_secret:
        description: "Name of CircleCi project environment variable that
          holds authentication client secret, a required application variable"
        type: env_var_name
      cloudgov_username:
        description: "Name of CircleCi project environment variable that
          holds deployer username for cloudgov space"
        type: env_var_name
      cloudgov_password:
        description: "Name of CircleCi project environment variable that
          holds deployer password for cloudgov space"
        type: env_var_name
      cloudgov_space:
        description: "Name of CircleCi project environment variable that
          holds name of cloudgov space to target for application deployment"
        type: env_var_name
      deploy_config_file:
        description: "Path to deployment configuration file"
        type: string
      session_secret:
        description: "Name of CircleCi project environment variable that
          holds session secret, a required application variable"
        type: env_var_name
      jwt_secret:
        description: "CircleCi project environment variable used by the backend
          token service for the email verification flow."
        type: env_var_name
      new_relic_license:
        description: "Name of CircleCI project environment variable that
          holds the New Relic License key, a required application variable"
        type: env_var_name
      hses_data_file_url:
        description: "Url to download HSES grants and recipient data from"
        type: env_var_name
      hses_data_username:
        description: "Username used to access the HSES grants and recipient data"
        type: env_var_name
      hses_data_password:
        description: "Password used to access the HSES grants and recipient data"
        type: env_var_name
      smtp_host:
        description: "SMTP server"
        type: env_var_name
      smtp_port:
        description: "SMTP port"
        type: env_var_name
      smtp_host_test:
        description: "SMTP server test"
        type: env_var_name
      smtp_port_test:
        description: "SMTP port test"
        type: env_var_name
      smtp_secure:
        description: "SMTP secure transport"
        type: env_var_name
      smtp_ignore_tls:
        description: "SMTP specifies whether to negotiate TLS"
        type: env_var_name
      from_email_address:
        description: "From email address"
        type: env_var_name
      smtp_user:
        description: "SMTP user"
        type: env_var_name
      smtp_password:
        description: "SMTP password"
        type: env_var_name
      suppress_error_logging:
        description: "Stop logging of non-sequelize errors to the db"
        type: env_var_name
      itams_md_host:
        description: "host url for itams monitoring data"
        type: env_var_name
      itams_md_port:
        description: "port for itams monitoring data"
        type: env_var_name
      itams_md_username:
        description: "username for itams monitoring data"
        type: env_var_name
      itams_md_password:
        description: "password for itams monitoring data"
        type: env_var_name
    steps:
      - run:
          name: Login with service account
          command: |
            cf login -a << pipeline.parameters.cg_api >> \
              -u ${<< parameters.cloudgov_username >>} \
              -p ${<< parameters.cloudgov_password >>} \
              -o << pipeline.parameters.cg_org >> \
              -s ${<< parameters.cloudgov_space >>}
      - run:
          name: Push application with deployment vars
          command: |
            cf push \
              --vars-file << parameters.deploy_config_file >> \
              --var AUTH_CLIENT_ID=${<< parameters.auth_client_id >>} \
              --var AUTH_CLIENT_SECRET=${<< parameters.auth_client_secret >>} \
              --var NEW_RELIC_LICENSE_KEY=${<< parameters.new_relic_license >>} \
              --var SESSION_SECRET=${<< parameters.session_secret >>} \
              --var JWT_SECRET=${<< parameters.jwt_secret >>} \
              --var HSES_DATA_FILE_URL=${<< parameters.hses_data_file_url >>} \
              --var HSES_DATA_USERNAME=${<< parameters.hses_data_username >>} \
              --var HSES_DATA_PASSWORD=${<< parameters.hses_data_password >>} \
              --var SMTP_HOST=${<< parameters.smtp_host >>} \
              --var SMTP_PORT=${<< parameters.smtp_port >>} \
              --var SMTP_HOST_TEST=${<< parameters.smtp_host_test >>} \
              --var SMTP_PORT_TEST=${<< parameters.smtp_port_test >>} \
              --var SMTP_USER=${<< parameters.smtp_user >>} \
              --var SMTP_PASSWORD=${<< parameters.smtp_password >>} \
              --var SMTP_SECURE=${<< parameters.smtp_secure >>} \
              --var SMTP_IGNORE_TLS=${<< parameters.smtp_ignore_tls >>} \
              --var FROM_EMAIL_ADDRESS=${<< parameters.from_email_address >>} \
              --var SUPPRESS_ERROR_LOGGING=${<< parameters.suppress_error_logging >>} \
              --var ITAMS_MD_HOST=${<< parameters.itams_md_host >>} \
              --var ITAMS_MD_PORT=${<< parameters.itams_md_port >>} \
              --var ITAMS_MD_USERNAME=${<< parameters.itams_md_username >>} \
              --var ITAMS_MD_PASSWORD=${<< parameters.itams_md_password >>}
      # - run:
      #     name: Push maintenance application
      #     command: |
      #       cd maintenance_page && cf push -s cflinuxfs4 --vars-file ../<<parameters.deploy_config_file >>
  cf_backup:
    description: "Login to cloud foundry space with service account credentials, Connect to DB & S3, backup DB to S3"
    parameters:
      app_name:
        description: "Name of Cloud Foundry cloud.gov application; must match
          application name specified in manifest"
        type: string
      auth_client_id:
        description: "Name of CircleCi project environment variable that
          holds authentication client id, a required application variable"
        type: env_var_name
      auth_client_secret:
        description: "Name of CircleCi project environment variable that
          holds authentication client secret, a required application variable"
        type: env_var_name
      cloudgov_username:
        description: "Name of CircleCi project environment variable that
          holds deployer username for cloudgov space"
        type: env_var_name
      cloudgov_password:
        description: "Name of CircleCi project environment variable that
          holds deployer password for cloudgov space"
        type: env_var_name
      cloudgov_space:
        description: "Name of CircleCi project environment variable that
          holds name of cloudgov space to target for application deployment"
        type: env_var_name
    steps:
      - run:
          name: Install Dependencies
          command: |
            set -x
            set -e
            set -u
            set -o pipefail
            # update
            sudo apt-get update
            # Install PostgreSQL Client to have access to pg_dump
            sudo apt-get install -y postgresql-client
            # Install jq
            sudo apt-get install -y jq
            # Install zip & unzip
            sudo apt-get install -y zip unzip
            # Install Curl
            sudo apt-get install -y curl
            # Install uuid-runtime to have access to uuidgen
            sudo apt-get install uuid-runtime
            # Install pv
            sudo apt-get install pv
            # Install wget
            sudo apt-get install wget
            # Install Cloud Foundry CLI
            wget -q -O - https://packages.cloudfoundry.org/debian/cli.cloudfoundry.org.key | sudo apt-key add -
            echo "deb https://packages.cloudfoundry.org/debian stable main" | sudo tee /etc/apt/sources.list.d/cloudfoundry-cli.list
            sudo apt-get update
            sudo apt-get install cf8-cli
            # Install plugin needed for connect-to-service
            cf install-plugin -f https://github.com/cloud-gov/cf-service-connect/releases/download/v1.1.3/cf-service-connect_linux_amd64

            # The line you want to ensure exists in the /etc/hosts file
            line="127.0.0.1        localhost"

            # Check if the line already exists
            if ! grep -qF "$line" /etc/hosts; then
                # If the line does not exist, append it
                echo "$line" | sudo tee -a /etc/hosts > /dev/null
                echo "Line added to /etc/hosts"
            else
                echo "Line already exists in /etc/hosts"
            fi

            # cleanup
            sudo rm -rf /var/lib/apt/lists/*
      - run:
          name: Login with service account
          command: |
            cf login -a << pipeline.parameters.cg_api >> \
              -u ${<< parameters.cloudgov_username >>} \
              -p ${<< parameters.cloudgov_password >>} \
              -o << pipeline.parameters.cg_org >> \
              -s ${<< parameters.cloudgov_space >>}
      - run:
          name: Prep S3 Connection
          command: |
            set -x
            set -e
            set -u
            set -o pipefail

            # Functions definition
            function check_dependencies() {
                local dependencies=("jq" "aws" "cf" "uuidgen")
                for dep in "${dependencies[@]}"; do
                    if ! type "${dep}" > /dev/null; then
                        echo "Dependency ${dep} is not installed." >&2
                        exit 1
                    fi
                done
            }

            # Generate a unique service key name using uuidgen
            KEY_NAME="${CF_S3_SERVICE_NAME}-key-$(uuidgen)"

            # Function to check if a service key exists
            function check_service_key_exists() {
                echo "Checking if service key ${KEY_NAME} exists..."
                if cf service-keys "${CF_S3_SERVICE_NAME}" | grep -q "${KEY_NAME}"; then
                    echo "Service key ${KEY_NAME} exists."
                    return 0
                else
                    echo "Service key ${KEY_NAME} does not exist."
                    return 2
                fi
            }

            # Create a service key
            function create_service_key() {
                if check_service_key_exists; then
                  echo "Service key with name ${KEY_NAME} already exists"
                else
                  echo "Creating service key with name ${KEY_NAME}..."
                  if ! cf create-service-key "${CF_S3_SERVICE_NAME}" "${KEY_NAME}"; then
                      echo "Failed to create service key." >&2
                      exit 3
                  elif ! check_service_key_exists; then
                      echo "Failed to create service key, even thou it returned." >&2
                      exit 4
                  fi
                fi
            }

            # Fetch the service key credentials and filter out the JSON part
            function fetch_service_key() {
                # Capture the entire output of the command, without echo commands affecting it
                full_output=$(cf service-key "${CF_S3_SERVICE_NAME}" "${KEY_NAME}" 2>&1)

                # Use awk to capture lines starting from the first '{' until the end of the file
                credentials_json=$(echo "${full_output}" | awk '/\{/,0')
                if [ -z "${credentials_json}" ]; then
                    echo "No JSON data found." >&2
                    exit 5
                fi

                # Output the filtered JSON, removing any extraneous output before it
                echo "${credentials_json}"
            }

            # Validate JSON
            function validate_json() {
                echo "Validating JSON..."
                if ! echo "$1" | jq empty 2>/dev/null; then
                    echo "Invalid JSON format." >&2
                    echo $1
                    exit 6
                fi
            }

            # Main execution
            check_dependencies

            create_service_key
            credentials_json=$(fetch_service_key)

            # Validate JSON before parsing
            validate_json "${credentials_json}"
            echo 'test.1'

            # Extract credentials using jq
            access_key=$(echo "${credentials_json}" | jq -r '.credentials.access_key_id')
            secret_key=$(echo "${credentials_json}" | jq -r '.credentials.secret_access_key')
            bucket_name=$(echo "${credentials_json}" | jq -r '.credentials.bucket')
            region=$(echo "${credentials_json}" | jq -r '.credentials.region')
            echo 'test.2'

            if [[ -z "${access_key}" || -z "${secret_key}" || -z "${bucket_name}" || -z "${region}" ]]; then
                echo "Failed to extract all necessary credentials." >&2
                exit 7
            fi
            echo 'test.3'

            # Export credentials and the service key name for use in this script and potentially others
            echo "export CF_S3_ACCESS_KEY=${access_key}" >> ${BASH_ENV}
            echo "export CF_S3_SECRET_KEY=${secret_key}" >> ${BASH_ENV}
            echo "export CF_S3_BUCKET_NAME=${bucket_name}" >> ${BASH_ENV}
            echo "export CF_S3_REGION=${region}" >> ${BASH_ENV}
            echo "export CF_S3_SERVICE_KEY_NAME=${KEY_NAME}" >> ${BASH_ENV}
            echo 'test.4'
          environment:
            auth_client_id: PROD_AUTH_CLIENT_ID
            auth_client_secret: PROD_AUTH_CLIENT_SECRET
            CF_APP_NAME: tta-smarthub-prod
            CF_S3_SERVICE_NAME: ttahub-db-backups
      - run:
          name: Connect to DB, dump, zip, upload, disconnect from DB
          command: |
            set -x
            set -e
            set -u
            set -o pipefail

            # Function to check for required dependencies
            function check_dependencies() {
                for dep in "$@"; do
                    if ! type "${dep}" > /dev/null; then
                        echo "Dependency ${dep} is not installed." >&2
                        exit 1
                    fi
                done
            }

            # Function to check and set the state of ssh access for CF_SERVICE_NAME
            function manageSpaceSSH() {
                # Check if CF_SERVICE_NAME is set and not empty
                if [[ -z "${CF_SERVICE_NAME}" ]]; then
                    echo "Error: CF_SERVICE_NAME is not set or empty."
                    return 1
                fi

                # Determine the action to take based on the argument
                local action="$1"
                local command=""
                local current_state_output=$(cf space-ssh-allowed "${CF_SERVICE_NAME}" 2>&1)
                local status=$?

                # Error handling for retrieving the current SSH state
                if [[ $status -ne 0 ]]; then
                    echo "Error retrieving current SSH status for ${CF_SERVICE_NAME}: $current_state_output"
                    return $status
                fi

                # Parsing the current state, assuming the command outputs a simple enable/disable status or similar
                local current_state=$(echo "$current_state_output" | grep -o 'enabled\|disabled')

                # Determine the desired command based on the action
                if [[ "$action" == "enable" && "$current_state" != "enabled" ]]; then
                    command="allow-space-ssh"
                elif [[ "$action" == "disable" && "$current_state" != "disabled" ]]; then
                    command="disallow-space-ssh"
                elif [[ "$current_state" == "enabled" && "$action" == "enable" ]] || [[ "$current_state" == "disabled" && "$action" == "disable" ]]; then
                    echo "SSH is already ${action}d for the service: ${CF_SERVICE_NAME}"
                    return 0
                else
                    echo "Invalid action specified. Use 'enable' or 'disable'."
                    return 3
                fi

                # Execute the cf command if necessary
                if [[ -n "$command" ]]; then
                    cf $command "${CF_SERVICE_NAME}"
                    local execute_status=$?

                    # Check the exit status of the cf command
                    if [ $execute_status -ne 0 ]; then
                        echo "Failed to ${action} SSH for the service: ${CF_SERVICE_NAME}"
                        return $execute_status
                    else
                        echo "SSH successfully ${action}d for the service: ${CF_SERVICE_NAME}"
                    fi
                fi
            }

            # Function to check if file size has stabilized for both output and error files
            function check_file_stabilization() {
                local output_file=${1}
                local error_file=${2}
                local last_output_size=0
                local last_error_size=0
                local current_output_size
                local current_error_size
                local max_checks=6 # Max checks at every 5 seconds gives us 30 seconds total

                for (( i=0; i<$max_checks; i++ )); do
                    cat ${output_file}
                    current_output_size=$(stat --format=%s "${output_file}" 2>/dev/null || echo 0)
                    current_error_size=$(stat --format=%s "${error_file}" 2>/dev/null || echo 0)

                    # Check if both files have stopped changing and are not empty
                    if [[ "$last_output_size" -eq "${current_output_size}" && "${last_error_size}" -eq "${current_error_size}" && "${current_output_size}" -ne 0 ]]; then
                        echo "Both output and error file sizes have stabilized."
                        return 0
                    fi

                    last_output_size=${current_output_size}
                    last_error_size=${current_error_size}
                    sleep 5
                done

                echo "Timeout reached without stabilization."
                return 1
            }

            function setup_cf_db_connection() {
                local cf_output=$(mktemp)
                local cf_error=$(mktemp)

                echo "Starting the connection setup..."
                # Execute cf connect-to-service and redirect output to a file, run in the background
                cf connect-to-service --no-client "${CF_APP_NAME}" "${CF_SERVICE_NAME}" > "${cf_output}" 2> "${cf_error}" &

                # Capture the PID of the cf connect-to-service process
                local CF_DB_CONNECTION_PID=$!
                echo "CF Connect to Service Process ID: ${CF_DB_CONNECTION_PID}"

                # Give some time for the process to start and produce output
                sleep 2

                # Wait for the output and error file sizes to stabilize
                if ! check_file_stabilization "${cf_output}" "${cf_error}"; then
                    echo "Failed to verify file stabilization." >&2
                    rm "${cf_output}" "${cf_error}"
                    exit 1
                fi

                # Check if the process is still running
                if ! ps -p "${CF_DB_CONNECTION_PID}" > /dev/null 2>/dev/null; then
                    echo "cf connect-to-service process has unexpectedly terminated." >&2
                    if [ -s "${cf_error}" ]; then
                        echo "Error output from cf connect-to-service:"
                        cat "${cf_error}" >&2
                    fi
                    rm "${cf_output}" "${cf_error}"
                    exit 1
                fi

                # Ensure the output file exists and is not empty before attempting to parse it
                if [ ! -f "${cf_output}" ] || [ ! -s "${cf_output}" ]; then
                    echo "No output file found or file is empty, cannot proceed with parsing." >&2
                    rm "${cf_output}" "${cf_error}"
                    exit 1
                fi

                # Extract credentials from the output file
                local HOST=$(grep 'Host:' "${cf_output}" | awk '{print $2}')
                local PORT=$(grep 'Port:' "${cf_output}" | awk '{print $2}')
                local USERNAME=$(grep 'Username:' "${cf_output}" | awk '{print $2}')
                local PASSWORD=$(grep 'Password:' "${cf_output}" | awk '{print $2}')
                local DB_NAME=$(grep 'Name:' "${cf_output}" | awk '{print $2}')

                # Check if any variable was not properly assigned
                if [[ -z "$HOST" || -z "$PORT" || -z "$USERNAME" || -z "$PASSWORD" || -z "$DB_NAME" ]]; then
                    echo "Failed to extract all necessary database connection credentials." >&2
                    rm "${cf_output}" "${cf_error}"
                    exit 2
                fi

                # Export them for use in this script
                echo "export CF_DB_HOST=${HOST}" >> "${BASH_ENV}"
                echo "export CF_DB_PORT=${PORT}" >> "${BASH_ENV}"
                echo "export CF_DB_USERNAME=${USERNAME}" >> "${BASH_ENV}"
                echo "export CF_DB_PASSWORD=${PASSWORD}" >> "${BASH_ENV}"
                echo "export CF_DB_NAME=${DB_NAME}" >> "${BASH_ENV}"

                # Store the PID in an environment variable for later use
                echo "export CF_DB_CONNECTION_PID=${CF_DB_CONNECTION_PID}" >> "${BASH_ENV}"

                # Clean up temporary files
                rm "${cf_output}" "${cf_error}"

                # Source the BASH_ENV to make the exported variables available immediately
                source "${BASH_ENV}"
            }

            # Terminate the database connection process if it's running
            function terminate_db_connection() {
                if ps -p ${CF_DB_CONNECTION_PID} > /dev/null 2>/dev/null; then
                    kill ${CF_DB_CONNECTION_PID}
                    echo "Database connection process terminated."
                else
                    echo "Database connection process was not running."
                fi
            }

            # Function to generate pre-signed S3 URLs for PUT operations
            generate_presigned_url() {
                local file_name=${1}
                local region=${2:-"default-region"}  # Use a default region if not specified
                local log_file=$(mktemp)  # Create a temporary file for logging

                # Ensure the temporary logfile is deleted upon function exit
                trap 'rm -f "${log_file}"' EXIT

                if [ -z "${file_name}" ]; then
                    echo "Error: No file name provided."
                    return 1
                fi

                # Corrected command to generate a presigned URL
                local url=$(aws s3 presign "s3://${CF_S3_BUCKET_NAME}/${file_name}" --expires-in 3600 --region ${region} 2>>${log_file})

                if [ $? -ne 0 ]; then
                    echo "Failed to generate presigned URL. Check the log file for details: ${log_file}"
                    cat ${log_file}  # Output the log file content for debugging
                    return 1
                fi

                echo $url
            }


            # Function to check if a file exists on S3
            file_exists_on_s3() {
                local file_name=${1}
                if aws s3 ls s3://${CF_S3_BUCKET_NAME}/${file_name} > /dev/null; then
                    return 0
                else
                    return 1
                fi
            }

            # Function to safely remove a file from S3 if it exists
            safe_remove_s3_file() {
                local file_name=${1}
                if file_exists_on_s3 ${file_name}; then
                    aws s3 rm s3://${CF_S3_BUCKET_NAME}/${file_name}
                fi
            }

            function generate_presigned_urls() {
                # Define S3 file names with precision down to the second, in UTC
                local TS="$(date --utc +%Y-%m-%d-%H-%M-%S)-UTC"
                local S3_FILE_NAME="production-${TS}.sql.zip"
                local PASSWORD_FILE_NAME="production-${TS}.sql.pwd"
                local LATEST_BACKUP_FILE_NAME="latest-backup.txt"

                # Generate UUID as the password
                local ZPASSWORD=$(uuidgen)

                # Export the ZIP password for external use
                echo "export ZIP_PASSWORD=${ZPASSWORD}" >> "${BASH_ENV}"

                # Configure AWS CLI
                export AWS_ACCESS_KEY_ID=${CF_S3_ACCESS_KEY}
                export AWS_SECRET_ACCESS_KEY=${CF_S3_SECRET_KEY}
                export AWS_DEFAULT_REGION=${CF_S3_REGION}

                # Use your existing function to generate presigned URLs
                local PRESIGNED_UPLOAD=$(generate_presigned_url "${S3_FILE_NAME}")
                local PRESIGNED_PASSWORD=$(generate_presigned_url "${PASSWORD_FILE_NAME}")
                local PRESIGNED_LATEST=$(generate_presigned_url "${LATEST_BACKUP_FILE_NAME}")

                # Export URL values for external use
                echo "export PRESIGNED_UPLOAD_URL=\"${PRESIGNED_UPLOAD}\"" >> "${BASH_ENV}"
                echo "export PRESIGNED_PASSWORD_URL=\"${PRESIGNED_PASSWORD}\"" >> "${BASH_ENV}"
                echo "export PRESIGNED_LATEST_URL=\"${PRESIGNED_LATEST}\"" >> "${BASH_ENV}"
            }

            function perform_backup_and_upload() {
                source "${BASH_ENV}"

                # Check required environment variables
                local required_vars=("CF_DB_PASSWORD" "CF_DB_USERNAME" "CF_DB_HOST" "CF_DB_PORT" "CF_DB_NAME" "ZIP_PASSWORD" "PRESIGNED_UPLOAD_URL" "PRESIGNED_PASSWORD_URL" "PRESIGNED_LATEST_URL" "S3_FILE_NAME" "PASSWORD_FILE_NAME")
                for var in "${required_vars[@]}"; do
                    if [ -z "${!var}" ]; then
                        echo "Error: Environment variable $var is not set."
                        return 1
                    fi
                done

                # Perform the backup and compress with the generated password
                PGPASSWORD=$CF_DB_PASSWORD pg_dump -U "${CF_DB_USERNAME}" -h "${CF_DB_HOST}" -p "${CF_DB_PORT}" "${CF_DB_NAME}" |\
                tee >(pv -n -brt --interval 30 > /dev/null) |\
                zip -P "${ZIP_PASSWORD}" - - |\
                curl -X PUT -T - -L "${PRESIGNED_UPLOAD_URL}" -o /dev/null

                # Evaluate the result of the backup and upload process
                if [ $? -ne 0 ]; then
                    echo "Backup upload failed."
                    safe_remove_s3_file "${S3_FILE_NAME}"
                    return 1
                fi

                echo "${ZIP_PASSWORD}" |\
                curl -X PUT -T - -L "${PRESIGNED_PASSWORD_URL}" -o /dev/null

                if [ $? -ne 0 ]; then
                    echo "Password file upload failed."
                    safe_remove_s3_file "${PASSWORD_FILE_NAME}"
                    return 1
                fi

                echo -e "${S3_FILE_NAME}\n${PASSWORD_FILE_NAME}" |\
                curl -X PUT -T - -L "${PRESIGNED_LATEST_URL}" -o /dev/null

                if [ $? -ne 0 ]; then
                    echo "Latest backup file list upload failed."
                    return 1
                fi

                echo "All files successfully uploaded."
            }

            # -----------------------------------------------------------------------------

            check_dependencies cf awk grep stat aws pg_dump zip curl uuidgen pv

            echo "enable ssh to db"
            manageSpaceSSH enable

            setup_cf_db_connection

            generate_presigned_urls

            perform_backup_and_upload

            terminate_db_connection
            manageSpaceSSH disable

            # Clean up
            rm ${cf_output}
            echo "Cleanup done."
          background: false
          environment:
            auth_client_id: PROD_AUTH_CLIENT_ID
            auth_client_secret: PROD_AUTH_CLIENT_SECRET
            CF_APP_NAME: tta-smarthub-prod
            CF_SERVICE_NAME: ttahub-prod
      - run:
          name: Cleanup and Terminate Connection
          command: |
            set -x
            set -e
            set -u
            set -o pipefail

            source ${BASH_ENV}

            # Function to check for required dependencies
            function check_dependencies() {
                local dependencies=("cf")
                for dep in "${dependencies[@]}"; do
                    if ! type "$dep" > /dev/null; then
                        echo "Dependency $dep is not installed." >&2
                        exit 1
                    fi
                done
            }

            # Function to check if a service key exists
            function check_service_key_exists() {
                echo "Checking if service key ${CF_S3_SERVICE_KEY_NAME} exists..."
                if cf service-keys "${CF_S3_SERVICE_NAME}" | grep -q "${CF_S3_SERVICE_KEY_NAME}"; then
                    echo "Service key ${CF_S3_SERVICE_KEY_NAME} exists."
                    return 0
                else
                    echo "Service key ${CF_S3_SERVICE_KEY_NAME} does not exist."
                    return 1
                fi
            }

            # Delete the S3 service key if deletion is allowed and the key exists
            function delete_service_key() {
                if [ "${DELETION_ALLOWED:-no}" = "yes" ]; then
                    if check_service_key_exists; then
                        echo "Deleting service key ${CF_S3_SERVICE_KEY_NAME}..."
                        if ! cf delete-service-key "$CF_S3_SERVICE_NAME" "${CF_S3_SERVICE_KEY_NAME}" -f; then
                            echo "Failed to delete service key ${CF_S3_SERVICE_KEY_NAME}." >&2
                            exit 2
                        fi
                    else
                        echo "No service key ${CF_S3_SERVICE_KEY_NAME} to delete."
                    fi
                else
                    echo "Service key deletion is not enabled. Set DELETION_ALLOWED=yes to enable."
                fi
            }

            # Main execution
            check_dependencies

            disableSpaceSSH
            delete_service_key
          when: always
          environment:
            CF_APP_NAME: tta-smarthub-prod
            CF_S3_SERVICE_NAME: ttahub-db-backups
      - run:
          name: Logout of service account
          command: |
            cf logout
parameters:
  cg_org:
    description: "Cloud Foundry cloud.gov organization name"
    default: "hhs-acf-ohs-tta"
    type: string
  cg_api:
    description: "URL of Cloud Controller in Cloud Foundry cloud.gov instance"
    default: "https://api.fr.cloud.gov"
    type: string
  prod_git_url:
    description: "URL of github repo that will deploy to prod"
    default: "https://github.com/HHS/Head-Start-TTADP"
    type: string
  staging_git_url:
    description: "URL of github repo that will deploy to staging"
    default: "https://github.com/HHS/Head-Start-TTADP"
    type: string
  dev_git_url:
    description: "URL of github repo that will deploy to dev"
    default: "https://github.com/HHS/Head-Start-TTADP"
    type: string
  sandbox_git_url:
    description: "URL of github repo that will deploy to sandbox"
    default: "https://github.com/HHS/Head-Start-TTADP"
    type: string
  prod_git_branch:
    description: "Name of github branch that will deploy to prod"
    default: "production"
    type: string
  staging_git_branch:
    description: "Name of github branch that will deploy to staging"
    default: "main"
    type: string
  dev_git_branch: # change to feature branch to test deployment
    description: "Name of github branch that will deploy to dev"
    default: "al/ttahub-2570/flat-resource-sql"
    type: string
  sandbox_git_branch:  # change to feature branch to test deployment
    default: "mb/TTAHUB/frontend-for-tr-dashboard"
    type: string
  prod_new_relic_app_id:
    default: "877570491"
    type: string
  staging_new_relic_app_id:
    default: "868729138"
    type: string
  dev_new_relic_app_id:
    default: "867221900"
    type: string
  sandbox_new_relic_app_id:
    default: "867346799"
    type: string
  manual-trigger:
    type: boolean
    default: false
jobs:
  build_and_lint:
    executor: docker-executor
    steps:
      - checkout
      - create_combined_yarnlock
      - restore_cache:
          keys:
            # To manually bust the cache, increment the version e.g. v7-yarn...
            - v14-yarn-deps-{{ checksum "combined-yarnlock.txt" }}
            # If checksum is new, restore partial cache
            - v14-yarn-deps-
      - run: yarn deps
      - save_cache:
          paths:
            - node_modules
            - frontend/node_modules
            - packages/common/node_modules
          key: v11-yarn-deps-{{ checksum "combined-yarnlock.txt" }}
      - run:
          name: Lint backend
          command: yarn lint:ci
      - run:
          name: Lint frontend
          command: yarn --cwd frontend lint:ci
      - store_artifacts:  # store backend lint reports
          path: reports
      - store_artifacts:  # store frontend lint reports
          path: frontend/reports
      - run:
          name: Remove similarity api data
          command: rm -rf similarity_api
      - persist_to_workspace:
          root: .
          paths:
            - .
    # The resource_class feature allows configuring CPU and RAM resources for each job. Different resource classes are available for different executors. https://circleci.com/docs/2.0/configuration-reference/#resourceclass
    resource_class: large
  build_and_lint_similarity_api:
    executor: docker-python-executor
    steps:
      - checkout
      - create_combined_pipfreeze
      - restore_cache:
          keys:
            # To manually bust the cache, increment the version e.g. v7-pip...
            - v2-pip-deps-{{ checksum "combined-requirements.txt" }}
            # If checksum is new, restore partial cache
            - v2-pip-deps-
      - run:
          name: Install python dependencies
          command: |
            cd similarity_api/src
            python3 -m venv venv
            source venv/bin/activate
            pip install -U pip setuptools wheel
            pip install -U -r requirements.txt
      - save_cache:
          paths:
            - similarity_api/src/venv
          key: v1-pip-deps-{{ checksum "combined-requirements.txt" }}
      - store_artifacts:  # store backend lint reports
          path: reports
      - store_artifacts:  # store frontend lint reports
          path: similarity_api/reports
      - persist_to_workspace:
          root: .
          paths:
            - similarity_api
    resource_class: large
  test_backend:
    executor: docker-postgres-elasticsearch-executor
    environment:
        AWS_ELASTICSEARCH_ENDPOINT: http://opensearch:9200
        AWS_ELASTICSEARCH_ACCESS_KEY: admin
        AWS_ELASTICSEARCH_SECRET_KEY: admin
        SFTP_EXPOSED_PORT: 2222
    steps:
      - attach_workspace:
          at: .
      - setup_remote_docker:
          version: default
      - run:
          name: Audit vulnerability of backend node_modules
          command: |
            chmod 744 ./run-yarn-audit.sh
            ./run-yarn-audit.sh;
      - run:
          name: Run migrations ci
          command: yarn db:migrate:ci
      - run:
          name: Run seeders
          command: yarn db:seed:ci
      - run:
          name: Monitor database
          command: |
            docker attach  $(docker ps | grep postgres | awk '{print $1}')
          background: true
      - run:
          name: Test backend
          command:  |
            chmod 744 ./bin/test-backend-ci
            ./bin/test-backend-ci
      - run:
          name: Compress coverage artifacts
          command: tar -cvzf backend-coverage-artifacts.tar coverage/
      - store_artifacts:
          path: coverage/
      - store_artifacts:
          path: backend-coverage-artifacts.tar
      - store_test_results:
          path: reports/
    resource_class: large
  test_similarity_api:
    executor: docker-python-executor
    steps:
      - attach_workspace:
          at: .
      - setup_remote_docker:
          version: default
      - run:
          name: Syft SBOM
          environment:
            SYFT_VERSION: v0.82.0
            IMAGE_NAME: ghcr.io/kcirtapfromspace/cloudfoundry_circleci:latest
            OUTPUT_FORMAT: json
            OUTPUT_FILE:  reports/syft_sbom.json
          command: |
            mkdir -p reports/
            curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b . "$SYFT_VERSION"
            ./syft similarity_api/src -vv --scope AllLayers -o "$OUTPUT_FORMAT" > "$OUTPUT_FILE"
            echo "scan results saved in $OUTPUT_FILE"
          # echo $GITHUB_PAT | ./syft login ghcr.io -u $GITHUB_USERNAME --password-stdin  -vv
          # echo $GITHUB_PAT | docker login ghcr.io -u $GITHUB_USERNAME --password-stdin
      - run:
          name: Grype Docker image
          environment:
            GRYPE_VERSION: v0.62.2
            OUTPUT_FORMAT: sarif
            OUTPUT_FILE: reports/grype.json
          command: |
            curl -sSfL https://raw.githubusercontent.com/anchore/grype/main/install.sh | sh -s -- -b . "$GRYPE_VERSION"
            ./grype sbom:reports/syft_sbom.json -v -o "$OUTPUT_FORMAT" > "$OUTPUT_FILE"
            echo "scan results saved in $OUTPUT_FILE"
      - run:
          name: Test similarity
          command: |
            mkdir -p coverage/similarity
            cd similarity_api/src
            source venv/bin/activate
            pip install pytest pytest-cov
            pytest -rpP --cov=similarity --cov=. --junitxml=~/project/reports/junit.xml
            coverage report --show-missing --skip-covered
            coverage html -d ~/project/coverage/similarity --skip-covered
      - store_artifacts:
          path: reports/
    resource_class: large
  test_frontend:
    executor: docker-executor
    steps:
      - attach_workspace:
          at: .
      - run:
          name: Audit checksums of color files
          command: |
            chmod 744 ./checkcolorhash.sh
            ./checkcolorhash.sh;
      - run:
          name: Audit vulnerability of frontend node_modules
          command: |
            cd frontend
            chmod 744 ./run-yarn-audit.sh
            ./run-yarn-audit.sh;
      - run:
          name: Test frontend
          command: yarn --cwd frontend run test:ci --maxWorkers=50%
      - store_test_results:
          path: frontend/reports/
      - store_artifacts:
          path: frontend/coverage/
    resource_class: large
  test_e2e:
    executor: docker-postgres-elasticsearch-executor
    environment:
        AWS_ELASTICSEARCH_ENDPOINT: http://opensearch:9200
        AWS_ELASTICSEARCH_ACCESS_KEY: admin
        AWS_ELASTICSEARCH_SECRET_KEY: admin
    steps:
      - attach_workspace:
          at: .
      - setup_remote_docker:
          version: default
      - run:
          name: Start server
          command: |
            yarn build
            BYPASS_AUTH=true CURRENT_USER_ID=5 yarn start:ci
          background: true
      - run:
          name: Run migrations ci
          command: yarn db:migrate:ci
      - run:
          name: Seed database
          command: yarn db:seed:ci
      - run:
          name: Wait for server to start
          command: ./bin/ping-server 3000
      - run:
          name: Install playwright dependencies
          command: |
            npx playwright install
      - run:
          name: Monitor database
          command: |
            docker attach  $(docker ps | grep postgres | awk '{print $1}')
          background: true
      - run:
          name: Run playwright tests
          command: yarn e2e:ci
      - store_artifacts:
          path: playwright/e2e
    resource_class: large
  test_api:
    executor: docker-postgres-elasticsearch-executor
    environment:
        AWS_ELASTICSEARCH_ENDPOINT: http://opensearch:9200
        AWS_ELASTICSEARCH_ACCESS_KEY: admin
        AWS_ELASTICSEARCH_SECRET_KEY: admin
    steps:
      - attach_workspace:
          at: .
      - setup_remote_docker:
          version: default
      - run:
          name: Start server
          command: |
            yarn build
            BYPASS_AUTH=true CURRENT_USER_ID=5 yarn start:ci
          background: true
      - run:
          name: Run migrations ci
          command: yarn db:migrate:ci
      - run:
          name: Seed database
          command: yarn db:seed:ci
      - run:
          name: Wait for server to start
          command: ./bin/ping-server 3000
      - run:
          name: Monitor database
          command: |
            docker attach  $(docker ps | grep postgres | awk '{print $1}')
          background: true
      - run:
          name: Install playwright dependencies
          command: |
            npx playwright install
      - run:
          name: Run playwright tests
          command: yarn e2e:api
      - store_artifacts:
          path: playwright/api
    resource_class: large
  test_utils:
    executor: docker-postgres-elasticsearch-executor
    environment:
        AWS_ELASTICSEARCH_ENDPOINT: http://opensearch:9200
        AWS_ELASTICSEARCH_ACCESS_KEY: admin
        AWS_ELASTICSEARCH_SECRET_KEY: admin
    steps:
      - attach_workspace:
          at: .
      - setup_remote_docker:
          version: default
      - run:
          name: Start server
          command: |
            yarn build
            BYPASS_AUTH=true CURRENT_USER_ID=5 yarn start:ci
          background: true
      - run:
          name: Run migrations ci
          command: yarn db:migrate:ci
      - run:
          name: Seed database
          command: yarn db:seed:ci
      - run:
          name: Wait for server to start
          command: ./bin/ping-server 3000
      - run:
          name: Install playwright dependencies
          command: |
            npx playwright install
      - run:
          name: Run playwright tests
          command: yarn e2e:utils
      - store_artifacts:
          path: playwright/utilsTests
    resource_class: large
  cucumber_test:
    executor: docker-postgres-executor
    steps:
      - attach_workspace:
          at: .
      - run:
          name: Start server
          command: |
            yarn build
            BYPASS_AUTH=true CURRENT_USER_ID=5 yarn start:ci
          background: true
      - run:
          name: Run migrations ci
          command: yarn db:migrate:ci
      - run:
          name: Seed database
          command: yarn db:seed:ci
      - run:
          name: Wait for server to start
          command: ./bin/ping-server 3000
      - run:
          name: Run cucumber
          command: |
            yarn cucumber:ci
      - store_artifacts:
          path: reports/
    resource_class: large
  dynamic_security_scan:
    executor: machine-executor
    steps:
      - attach_workspace:
          at: .
      - run:
          name: Start up local server
          command: ./bin/prod-style-server
      - run:
          name: Wait for server to start
          command: ./bin/ping-server 8080
      - run:
          name: Pull OWASP ZAP docker image
          command: docker pull softwaresecurityproject/zap-stable:latest
      - run:
          name: Make reports directory group writeable
          command: chmod g+w reports
      - run:
          name: Run OWASP ZAP
          command: ./bin/run-owasp-scan
      - store_artifacts:
          path: reports/owasp_report.html
    resource_class: arm.large
  deploy:
    executor: docker-executor
    steps:
      - attach_workspace:
          at: .
      - run:
          name: Build backend assets
          command: yarn build
      - when:
          condition:
            and:
              - equal: [<< pipeline.project.git_url >>, << pipeline.parameters.prod_git_url >>]
              - equal: [<< pipeline.git.branch >>, << pipeline.parameters.prod_git_branch >>]
          steps:
          - run:
              name: Create production robot
              command: ./bin/robot-factory
      - run:
          name: Install Cloud Foundry
          command: |
            curl -v -L -o cf-cli_amd64.deb 'https://packages.cloudfoundry.org/stable?release=debian64&version=v7&source=github'
            sudo dpkg -i cf-cli_amd64.deb
      - when:  # sandbox: for short-term feature development, see README.md
          condition:
            and:
              - equal: [<< pipeline.project.git_url >>, << pipeline.parameters.sandbox_git_url >>]
              - equal: [<< pipeline.git.branch >>, << pipeline.parameters.sandbox_git_branch >>]
          steps:
            - run:
                name: Build frontend assets
                command: yarn --cwd frontend run build
                environment:
                  REACT_APP_GTM_ENABLED: $SANDBOX_GTM_ENABLED
                  REACT_APP_GTM_ID: $GLOBAL_GTM_ID
                  REACT_APP_GTM_AUTH: $SANDBOX_GTM_AUTH
                  REACT_APP_GTM_PREVIEW: $SANDBOX_GTM_PREVIEW
                  REACT_APP_WEBSOCKET_URL: wss://tta-smarthub-sandbox.app.cloud.gov
            - cf_deploy:
                app_name: tta-smarthub-sandbox
                auth_client_id: SANDBOX_AUTH_CLIENT_ID
                auth_client_secret: SANDBOX_AUTH_CLIENT_SECRET
                cloudgov_username: CLOUDGOV_SANDBOX_USERNAME
                cloudgov_password: CLOUDGOV_SANDBOX_PASSWORD
                cloudgov_space: CLOUDGOV_SANDBOX_SPACE
                deploy_config_file: deployment_config/sandbox_vars.yml
                new_relic_license: NEW_RELIC_LICENSE_KEY
                session_secret: SANDBOX_SESSION_SECRET
                jwt_secret: SANDBOX_JWT_SECRET
                hses_data_file_url: HSES_DATA_FILE_URL
                hses_data_username: HSES_DATA_USERNAME
                hses_data_password: HSES_DATA_PASSWORD
                smtp_host: STAGING_SMTP_HOST
                smtp_port: STAGING_SMTP_PORT
                smtp_host_test: SMTP_HOST_TEST
                smtp_port_test: SMTP_PORT_TEST
                smtp_secure: SMTP_SECURE
                smtp_ignore_tls: STAGING_SMTP_IGNORE_TLS
                from_email_address: FROM_EMAIL_ADDRESS
                smtp_user: SMTP_USER
                suppress_error_logging: SUPPRESS_ERROR_LOGGING
                smtp_password: SMTP_PASSWORD
                itams_md_host: ITAMS_MD_HOST
                itams_md_port: ITAMS_MD_PORT
                itams_md_username: ITAMS_MD_USERNAME
                itams_md_password: ITAMS_MD_PASSWORD
            - run:
                name: Migrate database
                command: |
                  cf run-task tta-smarthub-sandbox \
                    --command "yarn db:migrate:prod" \
                    --name "Reset DB"
            - notify_new_relic:
                env_name: sandbox
                new_relic_app_id: << pipeline.parameters.sandbox_new_relic_app_id >>
                new_relic_api_key: $NEW_RELIC_REST_API_KEY
      - when:  # dev
          condition:
            and:
              - equal: [<< pipeline.project.git_url >>, << pipeline.parameters.dev_git_url >>]
              - equal: [<< pipeline.git.branch >>, << pipeline.parameters.dev_git_branch >>]
          steps:
            - run:
                name: Build frontend assets
                command: yarn --cwd frontend run build
                environment:
                  REACT_APP_GTM_ENABLED: $DEV_GTM_ENABLED
                  REACT_APP_GTM_ID: $GLOBAL_GTM_ID
                  REACT_APP_GTM_AUTH: $DEV_GTM_AUTH
                  REACT_APP_GTM_PREVIEW: $DEV_GTM_PREVIEW
                  REACT_APP_WEBSOCKET_URL: wss://tta-smarthub-dev.app.cloud.gov
            - cf_deploy:
                app_name: tta-smarthub-dev
                auth_client_id: DEV_AUTH_CLIENT_ID
                auth_client_secret: DEV_AUTH_CLIENT_SECRET
                cloudgov_username: CLOUDGOV_DEV_USERNAME
                cloudgov_password: CLOUDGOV_DEV_PASSWORD
                cloudgov_space: CLOUDGOV_DEV_SPACE
                deploy_config_file: deployment_config/dev_vars.yml
                new_relic_license: NEW_RELIC_LICENSE_KEY
                session_secret: DEV_SESSION_SECRET
                jwt_secret: DEV_JWT_SECRET
                hses_data_file_url: HSES_DATA_FILE_URL
                hses_data_username: HSES_DATA_USERNAME
                hses_data_password: HSES_DATA_PASSWORD
                smtp_host: STAGING_SMTP_HOST
                smtp_port: STAGING_SMTP_PORT
                smtp_host_test: SMTP_HOST_TEST
                smtp_port_test: SMTP_PORT_TEST
                smtp_secure: SMTP_SECURE
                smtp_ignore_tls: STAGING_SMTP_IGNORE_TLS
                from_email_address: FROM_EMAIL_ADDRESS
                smtp_user: SMTP_USER
                smtp_password: SMTP_PASSWORD
                suppress_error_logging: SUPPRESS_ERROR_LOGGING
                itams_md_host: ITAMS_MD_HOST
                itams_md_port: ITAMS_MD_PORT
                itams_md_username: ITAMS_MD_USERNAME
                itams_md_password: ITAMS_MD_PASSWORD
            - run:
                name: Migrate database
                command: |
                  cf run-task tta-smarthub-dev \
                    --command "yarn db:migrate:prod" \
                    --name "Reset DB"
            - notify_new_relic:
                env_name: dev
                new_relic_app_id: << pipeline.parameters.dev_new_relic_app_id >>
                new_relic_api_key: $NEW_RELIC_REST_API_KEY
      - when:  # staging
          condition:
            and:
              - equal: [<< pipeline.project.git_url >>, << pipeline.parameters.staging_git_url >>]
              - equal: [<< pipeline.git.branch >>, << pipeline.parameters.staging_git_branch >>]
          steps:
            - run:
                name: Build frontend assets
                command: yarn --cwd frontend run build
                environment:
                  REACT_APP_GTM_ENABLED: $STAGING_GTM_ENABLED
                  REACT_APP_GTM_ID: $GLOBAL_GTM_ID
                  REACT_APP_GTM_AUTH: $STAGING_GTM_AUTH
                  REACT_APP_GTM_PREVIEW: $STAGING_GTM_PREVIEW
                  REACT_APP_WEBSOCKET_URL: wss://tta-smarthub-staging.app.cloud.gov
            - cf_deploy:
                app_name: tta-smarthub-staging
                auth_client_id: STAGING_AUTH_CLIENT_ID
                auth_client_secret: STAGING_AUTH_CLIENT_SECRET
                cloudgov_username: CLOUDGOV_STAGING_USERNAME
                cloudgov_password: CLOUDGOV_STAGING_PASSWORD
                cloudgov_space: CLOUDGOV_STAGING_SPACE
                deploy_config_file: deployment_config/staging_vars.yml
                new_relic_license: NEW_RELIC_LICENSE_KEY
                session_secret: STAGING_SESSION_SECRET
                jwt_secret: STAGING_JWT_SECRET
                hses_data_file_url: HSES_DATA_FILE_URL
                hses_data_username: HSES_DATA_USERNAME
                hses_data_password: HSES_DATA_PASSWORD
                smtp_host: STAGING_SMTP_HOST
                smtp_port: STAGING_SMTP_PORT
                smtp_host_test: SMTP_HOST_TEST
                smtp_port_test: SMTP_PORT_TEST
                smtp_secure: SMTP_SECURE
                smtp_ignore_tls: STAGING_SMTP_IGNORE_TLS
                from_email_address: FROM_EMAIL_ADDRESS
                smtp_user: SMTP_USER
                smtp_password: SMTP_PASSWORD
                suppress_error_logging: SUPPRESS_ERROR_LOGGING
                itams_md_host: ITAMS_MD_HOST
                itams_md_port: ITAMS_MD_PORT
                itams_md_username: ITAMS_MD_USERNAME
                itams_md_password: ITAMS_MD_PASSWORD
            - run:
                name: Run database migrations
                command: |
                  cf run-task tta-smarthub-staging --command "yarn db:migrate:prod" --name migrate
            - notify_new_relic:
                env_name: staging
                new_relic_app_id: << pipeline.parameters.staging_new_relic_app_id >>
                new_relic_api_key: $NEW_RELIC_REST_API_KEY
      - when:  # prod
          condition:
            and:
              - equal: [<< pipeline.project.git_url >>, << pipeline.parameters.prod_git_url >>]
              - equal: [<< pipeline.git.branch >>, << pipeline.parameters.prod_git_branch >>]
          steps:
            - run:
                name: Build frontend assets
                command: yarn --cwd frontend run build
                environment:
                  REACT_APP_GTM_ENABLED: $PROD_GTM_ENABLED
                  REACT_APP_GTM_ID: $GLOBAL_GTM_ID
                  REACT_APP_GTM_AUTH: $PROD_GTM_AUTH
                  REACT_APP_GTM_PREVIEW: $PROD_GTM_PREVIEW
                  REACT_APP_WEBSOCKET_URL: wss://ttahub.ohs.acf.hhs.gov
            - cf_deploy:
                app_name: tta-smarthub-prod
                auth_client_id: PROD_AUTH_CLIENT_ID
                auth_client_secret: PROD_AUTH_CLIENT_SECRET
                cloudgov_username: CLOUDGOV_PROD_USERNAME
                cloudgov_password: CLOUDGOV_PROD_PASSWORD
                cloudgov_space: CLOUDGOV_PROD_SPACE
                deploy_config_file: deployment_config/prod_vars.yml
                new_relic_license: NEW_RELIC_LICENSE_KEY
                session_secret: PROD_SESSION_SECRET
                jwt_secret: PROD_JWT_SECRET
                hses_data_file_url: PROD_HSES_DATA_FILE_URL
                hses_data_username: PROD_HSES_DATA_USERNAME
                hses_data_password: PROD_HSES_DATA_PASSWORD
                smtp_host: SMTP_HOST
                smtp_port: SMTP_PORT
                smtp_host_test: SMTP_HOST_TEST
                smtp_port_test: SMTP_PORT_TEST
                smtp_secure: SMTP_SECURE
                smtp_ignore_tls: SMTP_IGNORE_TLS
                from_email_address: FROM_EMAIL_ADDRESS
                smtp_user: SMTP_USER
                smtp_password: SMTP_PASSWORD
                suppress_error_logging: SUPPRESS_ERROR_LOGGING
                itams_md_host: ITAMS_MD_HOST
                itams_md_port: ITAMS_MD_PORT
                itams_md_username: ITAMS_MD_USERNAME
                itams_md_password: ITAMS_MD_PASSWORD
            - run:
                name: Run database migrations
                command: |
                  cf run-task tta-smarthub-prod --command "yarn db:migrate:prod" --name migrate
            - notify_new_relic:
                env_name: prod
                new_relic_app_id: << pipeline.parameters.prod_new_relic_app_id >>
                new_relic_api_key: $NEW_RELIC_REST_API_KEY
            - notify_slack:
                slack_bot_token: $SLACK_BOT_TOKEN
                slack_channel: "acf-ohs-ttahub--contractor-customer-team"
    resource_class: large
  backup_upload:
    executor: aws-executor
    steps:
      - cf_backup:
          app_name: tta-smarthub-prod
          auth_client_id: PROD_AUTH_CLIENT_ID
          auth_client_secret: PROD_AUTH_CLIENT_SECRET
          cloudgov_username: CLOUDGOV_PROD_USERNAME
          cloudgov_password: CLOUDGOV_PROD_PASSWORD
          cloudgov_space: CLOUDGOV_PROD_SPACE
workflows:
  build_test_deploy:
    jobs:
      - build_and_lint
      - build_and_lint_similarity_api
      - test_backend:
          requires:
            - build_and_lint
      - test_frontend:
          requires:
            - build_and_lint
      - test_e2e:
          requires:
            - build_and_lint
      - test_api:
          requires:
            - build_and_lint
      - test_similarity_api:
          requires:
            - build_and_lint_similarity_api
      - test_utils:
          requires:
            - build_and_lint
      - cucumber_test:
          requires:
            - build_and_lint
      - dynamic_security_scan:
          requires:
            - build_and_lint
      - deploy:
          requires:
            - test_backend
            - test_frontend
            - test_e2e
            - test_api
            - test_similarity_api
            - test_utils
            - cucumber_test
            - dynamic_security_scan
          filters:
            branches:
              only:
                - << pipeline.parameters.sandbox_git_branch >>
                - << pipeline.parameters.dev_git_branch >>
                - << pipeline.parameters.staging_git_branch >>
                - << pipeline.parameters.prod_git_branch >>
  daily_scan:
    triggers:
      - schedule:
          cron: "0 12 * * 1-5"
          filters:
            branches:
              only:
                - << pipeline.parameters.dev_git_branch >>
                - << pipeline.parameters.staging_git_branch >>
                - << pipeline.parameters.prod_git_branch >>
    jobs:
      - build_and_lint
      - build_and_lint_similarity_api
      - test_backend:
          requires:
            - build_and_lint
      - test_frontend:
          requires:
            - build_and_lint
      - test_e2e:
          requires:
            - build_and_lint
      - test_api:
          requires:
            - build_and_lint
      - test_similarity_api:
          requires:
            - build_and_lint_similarity_api
      - test_utils:
          requires:
            - build_and_lint
      - dynamic_security_scan:
          requires:
            - build_and_lint
  daily_backup_upload:
    triggers:
      - schedule:
          cron: "0 10 * * 1-5"
          filters:
            branches:
              only:
                - << pipeline.parameters.prod_git_branch >>
    jobs:
      - backup_upload
  manual_backup_upload:
    when:
      equal: [true, << pipeline.parameters.manual-trigger >>]
    jobs:
      - backup_upload
